<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> - Articles</title>
    <description>Deep Learning Engineer - Personal Blog</description>
    <link>
    </link>
    
      
      <item>
        <title>Achieving Generalizable Robustness of Deep Neural Networks by Stability Training</title>
        
          <description>&lt;p&gt;We study the recently introduced stability training as a general-purpose method to increase the robustness of deep neural networks against input perturbations. In particular, we explore its use as an alternative to data augmentation and validate its performance against a number of distortion types and transformations including adversarial examples. In our image classification experiments using ImageNet data stability training performs on a par or even outperforms data augmentation for specific transformations, while consistently offering improved robustness against a broader range of distortion strengths and types unseen during training, a considerably smaller hyperparameter dependence and less potentially negative side effects compared to data augmentation. This work was first documented in my &lt;a href=&quot;assets/Master_Thesis.pdf&quot;&gt;Masterâ€™s Thesis&lt;/a&gt; and led to a paper on the German Conference on Pattern Recognition &lt;a href=&quot;https://arxiv.org/abs/1906.00735&quot;&gt;arXiv link&lt;/a&gt;.&lt;/p&gt;
</description>
        
        <pubDate>Mon, 03 Jun 2019 10:00:00 +0000</pubDate>
        <link>
        /stability-paper</link>
        <guid isPermaLink="true">/stability-paper</guid>
      </item>
      
    
  </channel>
</rss>
